{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infant Mortality\n",
    "\n",
    "One of my primary tasks at the Indiana Department of Health was, given the data from a child's birth certificate, determine the probability of their death.  After linking the birth and death certificates, this involved simply giving a report of the descriptive statitistics of infant deaths (defined as deaths of children who were born who died one year or younger) followed by a logistic regression.  I was never asked to make a more detailed predictive model, but as this is a project for my portfolio and not for my job, we can have a little fun.  Below I will first do a logistic regression, and then try a decision tree, a neural network, and a random forest.  *Note to readers:  This is a work in progress.  This page will be updated weekly.*\n",
    "\n",
    "\n",
    "While the data from IDOH are de-identified and I could conceivably use those, I'll elect to make my own dataset by splitting ten thousand individuals between three races (purple, blue, and green), two genders (positive and negative), and four ages (young, teen, adult, senior).\n",
    "\n",
    "But first, let's import some necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Population and Assign Probabilities\n",
    "\n",
    "In the next session we'll create our population and assign the probabilities of death.\n",
    "\n",
    "### Create Population\n",
    "\n",
    "As mentioned above, we'll have three races (purple, blue, and green), four ages (young, teen, adult, senior), and at risk of reinforcing the gender binary, two genders (positive and negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# population will be 70% blue, 20% green, 10% purple.\n",
    "race_list = [\"blue\"]*7000\n",
    "for i in range(2000):\n",
    "    race_list.append(\"green\")\n",
    "for i in range(1000):\n",
    "    race_list.append(\"purple\")\n",
    "\n",
    "# population will be 50% positive and 50% negative.\n",
    "gender_list = [\"positive\",\"negative\"]*5000\n",
    "\n",
    "# population will be 20% young, adult, and senior, and 40% teen.\n",
    "age_list = [\"young\",\"teen\",\"teen\",\"adult\",\"senior\"]*2000\n",
    "\n",
    "# assemble these lists into a data frame using the dictionary method:\n",
    "population = pd.DataFrame(\n",
    "                            { 'race':race_list,\n",
    "                              'gender':gender_list,\n",
    "                              'age':age_list }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Probabilities\n",
    "\n",
    "We'll make the probability of death 50% for purple, 10% for green, and 90% for purple.  Note that these are *much higher* than the probabilities of *actual* infant death, but for the purposes of this exercise we want to refrain from having too unbalanced a target set.  Just trying to show basic technique, here, and not dig into the specifics of how to deal with unbalanced sets.  (Perhaps that'll be a project for later.)\n",
    "\n",
    "At any rate, the probability for positive gender will be 70%, for negative gender will be 30%, and for young %40, 60% for teenagers, 80% for adult, and 10% for seniors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>total_probs</th>\n",
       "      <th>final_outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blue</td>\n",
       "      <td>positive</td>\n",
       "      <td>young</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blue</td>\n",
       "      <td>negative</td>\n",
       "      <td>teen</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blue</td>\n",
       "      <td>positive</td>\n",
       "      <td>teen</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blue</td>\n",
       "      <td>negative</td>\n",
       "      <td>adult</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blue</td>\n",
       "      <td>positive</td>\n",
       "      <td>senior</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   race    gender     age  total_probs  final_outcome\n",
       "0  blue  positive   young         0.27              0\n",
       "1  blue  negative    teen         0.32              0\n",
       "2  blue  positive    teen         0.36              0\n",
       "3  blue  negative   adult         0.36              1\n",
       "4  blue  positive  senior         0.45              0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign race probabilities\n",
    "race_probs = [0.5]*7000\n",
    "for i in range(2000):\n",
    "    race_probs.append(0.1)\n",
    "for i in range(1000):\n",
    "    race_probs.append(1.0)\n",
    "race_probs = np.array(race_probs)\n",
    "\n",
    "# assign gender probabilities\n",
    "gender_probs = [0.9,0.8]*5000\n",
    "gender_probs = np.array(gender_probs)\n",
    "\n",
    "# assign age probabilities\n",
    "age_probs = [0.6, 0.8, 0.8, 0.9, 1.0]*2000\n",
    "age_probs = np.array(age_probs)\n",
    "\n",
    "# figure out total probabilities, add column to data frame\n",
    "total_probs = race_probs*gender_probs*age_probs\n",
    "population['total_probs'] = total_probs\n",
    "\n",
    "# now assign each individual to an outcome, based on probability\n",
    "final_outcome = np.array( [0]*10000 )\n",
    "for i in range(10000):\n",
    "    j = random.random()\n",
    "    if j > total_probs[i]:\n",
    "        final_outcome[i] = 0\n",
    "    else:\n",
    "        final_outcome[i] = 1\n",
    "population['final_outcome'] = final_outcome\n",
    "\n",
    "# take a look at the population\n",
    "population.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics\n",
    "\n",
    "Let's do some quick and dirty descriptive statistics on the dataset.  We have already determined the proportion of race, age, and gender, so we'll not worry about that.  How many are dead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dead: 3243\n",
      "Percentage of population 32.43\n"
     ]
    }
   ],
   "source": [
    "x = population['final_outcome'].sum()\n",
    "N = 10000\n",
    "\n",
    "print(\"Number of dead:\",x)\n",
    "print(\"Percentage of population\",100*x/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   race    gender     age\n",
      "0  blue  negative   adult\n",
      "1  blue  negative    teen\n",
      "2  blue  negative  senior\n",
      "3  blue  negative    teen\n",
      "4  blue  positive  senior\n",
      "Number of dead: 3243\n",
      "Total population breakdown:\n",
      "race    gender    age   \n",
      "blue    positive  teen      0.155103\n",
      "        negative  teen      0.131668\n",
      "        positive  senior    0.096516\n",
      "                  adult     0.089732\n",
      "        negative  senior    0.083873\n",
      "                  adult     0.074931\n",
      "        positive  young     0.055813\n",
      "        negative  young     0.053962\n",
      "purple  positive  teen      0.043478\n",
      "        negative  teen      0.038853\n",
      "        positive  adult     0.028060\n",
      "                  senior    0.027752\n",
      "        negative  senior    0.025285\n",
      "                  adult     0.022202\n",
      "                  young     0.015418\n",
      "        positive  young     0.013568\n",
      "green   negative  teen      0.008326\n",
      "        positive  teen      0.007092\n",
      "        negative  senior    0.006475\n",
      "        positive  adult     0.005550\n",
      "                  senior    0.004934\n",
      "        negative  adult     0.004625\n",
      "        positive  young     0.003700\n",
      "        negative  young     0.003084\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Proportion race:\n",
      "race\n",
      "blue      0.741597\n",
      "purple    0.214616\n",
      "green     0.043787\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Proportion gender:\n",
      "gender\n",
      "positive    0.531298\n",
      "negative    0.468702\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Proportion age:\n",
      "age\n",
      "teen      0.384521\n",
      "senior    0.244835\n",
      "adult     0.225100\n",
      "young     0.145544\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# create a data frame of the deceased population consisting of race, gender, and age columns\n",
    "pop_deceased = population[population['final_outcome']==1].reset_index(drop=True).drop(['total_probs','final_outcome'],axis=1)\n",
    "print(pop_deceased.head())\n",
    "\n",
    "N = len(pop_deceased)\n",
    "print(\"Number of dead:\",N)\n",
    "\n",
    "print(\"Total population breakdown:\")\n",
    "print( pop_deceased.value_counts()/N )\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Proportion race:\")\n",
    "print( pop_deceased.value_counts('race')/N )\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Proportion gender:\")\n",
    "print( pop_deceased.value_counts('gender')/N )\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Proportion age:\")\n",
    "print( pop_deceased.value_counts('age')/N )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 74.7% of the deceased are blue race, 21.1% are purple race, and 4.1% are green race.  52.3% are positive gender, 47.7% negative gender.  14.1% are young, 39.5% are teens, 21.8% are adults, 24.7% are seniors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Now let's perform a logistic regression on the dataset, the first and simplest form of machine learning we can apply here.\n",
    "\n",
    "Because we're working with categorical data, we can't simply plug the numbers in directly and let 'er rip.  Insetad we need to encode them into dummy variables.  For this we need LabelEncoder() and OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>total_probs</th>\n",
       "      <th>final_outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      race  gender  age  total_probs  final_outcome\n",
       "0        0       1    3         0.27              0\n",
       "1        0       0    2         0.32              0\n",
       "2        0       1    2         0.36              0\n",
       "3        0       0    0         0.36              1\n",
       "4        0       1    1         0.45              0\n",
       "...    ...     ...  ...          ...            ...\n",
       "9995     2       0    3         0.48              1\n",
       "9996     2       1    2         0.72              1\n",
       "9997     2       0    2         0.64              1\n",
       "9998     2       1    0         0.81              1\n",
       "9999     2       0    1         0.80              0\n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = population.select_dtypes(exclude=['number']) \\\n",
    "              .apply(LabelEncoder().fit_transform) \\\n",
    "              .join(population.select_dtypes(include=['number']))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in our first step, we have encoded race so that blue is 0, green is 1, and purple is 2.  Gender is encoded so that 0 is negative and 1 is positive.  Age is encoded so that 0 is adult, 1 is senior, 2 is teen, and 3 is young.\n",
    "\n",
    "Next we use one hot encoder and run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.38089533]\n",
      "[[ 0.01692769 -1.91738952  1.5195665  -0.29782757 -0.08306775  0.11016506\n",
      "   0.26683522 -0.16277802 -0.59511758]]\n"
     ]
    }
   ],
   "source": [
    "pop_subset = x[['race','gender','age']]\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(pop_subset)\n",
    "onehotlabels = enc.transform(pop_subset).toarray()\n",
    "\n",
    "model = LogisticRegression(solver='liblinear',random_state=0) \\\n",
    "        .fit(onehotlabels,x['final_outcome'])\n",
    "\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have our model, approximately:\n",
    "$$y = -0.39 + 0.05\\beta_{blue} - 1.96\\beta_{green} + 1.52\\beta_{purple} - 0.276\\beta_{negative} -0.12\\beta_{positive} + 0.07\\beta_{adult} + 0.29\\beta_{senior} - 0.11\\beta_{teen} - 0.64\\beta_{young}$$\n",
    "\n",
    "How does our model do at predicting?  Let's make a \"classification report\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.96      0.82      6757\n",
      "           1       0.70      0.21      0.33      3243\n",
      "\n",
      "    accuracy                           0.71     10000\n",
      "   macro avg       0.71      0.58      0.57     10000\n",
      "weighted avg       0.71      0.71      0.66     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(onehotlabels)\n",
    "\n",
    "print( classification_report(x['final_outcome'],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6453,  304],\n",
       "       [2547,  696]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(x['final_outcome'],y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, 6,417 true negatives, 694 true positives, 306 false negatives, and 2,583 false positives.\n",
    "\n",
    "Let's see if we can't do better!\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "Now let's try a decision tree, and see if that improves our results at all.  We'll try two methods.  Training it according to the Gini Index $1-\\sum_j j^2$ and the entropy $-\\sum_{i=1}^N p(x_i) log_2 p(x_i)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_leaf=50,\n",
       "                       random_state=100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GiniTree = DecisionTreeClassifier(criterion = \"gini\", random_state = 100, max_depth = 5, min_samples_leaf=50)\n",
    "GiniTree.fit(onehotlabels,x['final_outcome'])\n",
    "\n",
    "EntropyTree = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth = 5, min_samples_leaf=50)\n",
    "EntropyTree.fit(onehotlabels,x['final_outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.97      0.82      6757\n",
      "           1       0.75      0.19      0.30      3243\n",
      "\n",
      "    accuracy                           0.72     10000\n",
      "   macro avg       0.73      0.58      0.56     10000\n",
      "weighted avg       0.73      0.72      0.65     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gini_pred = GiniTree.predict(onehotlabels)\n",
    "\n",
    "print( classification_report(x['final_outcome'],gini_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.97      0.82      6757\n",
      "           1       0.75      0.19      0.30      3243\n",
      "\n",
      "    accuracy                           0.72     10000\n",
      "   macro avg       0.73      0.58      0.56     10000\n",
      "weighted avg       0.73      0.72      0.65     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entr_pred = EntropyTree.predict(onehotlabels)\n",
    "\n",
    "print( classification_report(x['final_outcome'],entr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6559,  198],\n",
       "       [2641,  602]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(x['final_outcome'],gini_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6559,  198],\n",
       "       [2641,  602]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(x['final_outcome'],entr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we basically see no improvement using a decision tree over using a simple logistic regression.\n",
    "\n",
    "*Next step:  Neural network.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
